{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b49282f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import timedelta\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data import ComicsDataset, ComicPanelBatch\n",
    "from models.lstm import TextOnlyHeirarchicalLSTM\n",
    "from models.transformer_baselines import TextOnlyTransformerBaseline\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def collate_fn(batches_and_labels: List[List[Tuple[ComicPanelBatch, torch.Tensor]]]):\n",
    "    \"\"\"\n",
    "    Dummy collate function to just return the single element from the list (which will\n",
    "    already be a list of batches), since our dataset does the batching logic.\n",
    "    \"\"\"\n",
    "    assert len(batches_and_labels) == 1\n",
    "    return batches_and_labels[0]\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scaler,\n",
    "    pbar_total: int,\n",
    "    pbar_step: int,\n",
    "    pbar_desc: str,\n",
    "    iters_to_accumulate: int = 1,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "\n",
    "    with tqdm(leave=False, total=pbar_total, desc=pbar_desc) as pbar:\n",
    "        for batches in dataloader:\n",
    "            for batch, labels in batches:\n",
    "                batch.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    logits = model(batch)\n",
    "\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "                loss /= iters_to_accumulate\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                n_batches += 1\n",
    "\n",
    "                # Accumulates scaled gradients.\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "                if n_batches % iters_to_accumulate == 0:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # loss.backward()\n",
    "                # optimizer.step()\n",
    "            pbar.update(pbar_step)\n",
    "\n",
    "    avg_loss = total_loss / n_batches\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def eval_one_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    pbar_total: int,\n",
    "    pbar_step: int,\n",
    "    pbar_desc: str,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        n_batches = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with tqdm(leave=False, total=pbar_total, desc=pbar_desc) as pbar:\n",
    "            for batches in dataloader:\n",
    "                for batch, labels in batches:\n",
    "                    batch.to(device, non_blocking=True)\n",
    "                    labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        logits = model(batch)\n",
    "\n",
    "                    loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "                    preds = torch.argmax(logits, dim=-1)\n",
    "                    all_preds.append(preds.cpu())\n",
    "                    all_labels.append(labels.cpu())\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    n_batches += 1\n",
    "                pbar.update(pbar_step)\n",
    "\n",
    "        avg_loss = total_loss / n_batches\n",
    "\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        n_examples = all_preds.size(0)\n",
    "        acc = (torch.sum(all_preds == all_labels) / n_examples).item()\n",
    "\n",
    "        return avg_loss, acc, all_preds, all_labels\n",
    "\n",
    "\n",
    "def make_dataloader(\n",
    "    comics_data_path: str,\n",
    "    vgg_feats_path: str,\n",
    "    vocab_path: str,\n",
    "    folds_dir: str,\n",
    "    fold: str,\n",
    "    difficulty: str,\n",
    "    megabatch_size: int,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    pin_memory: bool,\n",
    "):\n",
    "    dataset = ComicsDataset(\n",
    "        comics_data_path=comics_data_path,\n",
    "        vgg_feats_path=vgg_feats_path,\n",
    "        vocab_path=vocab_path,\n",
    "        folds_dir=folds_dir,\n",
    "        difficulty=difficulty,\n",
    "        fold=fold,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    # We use SequentialSampler because the original code did not shuffle example order,\n",
    "    # and we use BatchSampler to pass multiple indices to our dataset.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        sampler=BatchSampler(\n",
    "            SequentialSampler(dataset), batch_size=megabatch_size, drop_last=False\n",
    "        ),\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return dataloader, dataset\n",
    "\n",
    "\n",
    "def main(\n",
    "    comics_data_path: str = '../11711_COMICS/data/comics.h5',\n",
    "    vgg_feats_path: str = '../11711_COMICS/data/vgg_features.h5',\n",
    "    vocab_path: str = '../11711_COMICS/data/comics_vocab.p',\n",
    "    folds_dir: str = '../11711_COMICS/folds',\n",
    "    difficulty: str = 'easy',\n",
    "    n_epochs: int = 10,\n",
    "    megabatch_size: int = 512,\n",
    "    batch_size: int = 16,\n",
    "    iters_to_accumulate: int = 4,\n",
    "    num_workers: int = 16,\n",
    "    lr: float = 1e-6,  # Small learning rate for finetuning.\n",
    "    show_tqdm: bool = False,\n",
    "):\n",
    "    # NOTE: Need to pass bytes as the encoding scheme here, there seems to be some\n",
    "    # incompability between python 2/3 pickle. For more info see:\n",
    "    # https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3\n",
    "    word_to_idx, idx_to_word = pickle.load(open(vocab_path, 'rb'), encoding='bytes')\n",
    "\n",
    "    data_kwargs = {\n",
    "        'comics_data_path': comics_data_path,\n",
    "        'vgg_feats_path': vgg_feats_path,\n",
    "        'vocab_path': vocab_path,\n",
    "        'folds_dir': folds_dir,\n",
    "        'difficulty': difficulty,\n",
    "        'megabatch_size': megabatch_size,\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': True,\n",
    "    }\n",
    "    train_dataloader, train_dataset = make_dataloader(**data_kwargs, fold='train')\n",
    "    valid_dataloader, valid_dataset = make_dataloader(**data_kwargs, fold='dev')\n",
    "    test_dataloader, test_dataset = make_dataloader(**data_kwargs, fold='test')\n",
    "\n",
    "    n_train_pages = len(train_dataset)\n",
    "    n_valid_pages = len(valid_dataset)\n",
    "    n_test_pages = len(test_dataset)\n",
    "\n",
    "    # Predefined parameters.\n",
    "    # total_pages, max_panels, max_boxes, max_words = train_data.words.shape\n",
    "    vocab_len = len(word_to_idx)\n",
    "\n",
    "    model = TextOnlyTransformerBaseline(idx_to_word)\n",
    "    # model = TextOnlyHeirarchicalLSTM(vocab_len)\n",
    "    model.to(device)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'{n_params} parameters.')\n",
    "    print(f'Using an effective batch size of {batch_size * iters_to_accumulate}.')\n",
    "    print(f'Using a learning rate of {lr}.')\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc='epochs', disable=not show_tqdm):\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            scaler,\n",
    "            pbar_total=n_train_pages,\n",
    "            pbar_step=megabatch_size,\n",
    "            pbar_desc='Train pages',\n",
    "            iters_to_accumulate=iters_to_accumulate,\n",
    "        )\n",
    "        valid_loss, valid_acc, _, _ = eval_one_epoch(\n",
    "            model,\n",
    "            valid_dataloader,\n",
    "            pbar_total=n_valid_pages,\n",
    "            pbar_step=megabatch_size,\n",
    "            pbar_desc='Valid. pages',\n",
    "        )\n",
    "        test_loss, test_acc, test_preds, test_labels = eval_one_epoch(\n",
    "            model,\n",
    "            test_dataloader,\n",
    "            pbar_total=n_test_pages,\n",
    "            pbar_step=megabatch_size,\n",
    "            pbar_desc='Test Pages',\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        duration = str(timedelta(seconds=end - start)).split('.')[0]\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch}: {train_loss}, {valid_loss}, {valid_acc}, {test_loss}, {test_acc}. Took {duration}s.'\n",
    "        )\n",
    "\n",
    "    # test_loss, test_acc, test_preds, test_labels = eval_one_epoch(\n",
    "    #     model, test_dataloader\n",
    "    # )\n",
    "    # print(f'{test_loss=:.4f}, {test_acc=:.4f}')\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     main(**vars(parser.parse_args()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026ee661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    comics_data_path: str = '../11711_COMICS/data/comics.h5',\n",
    "    vgg_feats_path: str = '../11711_COMICS/data/vgg_features.h5',\n",
    "    vocab_path: str = '../11711_COMICS/data/comics_vocab.p',\n",
    "    folds_dir: str = '../11711_COMICS/folds',\n",
    "    difficulty: str = 'easy',\n",
    "    n_epochs: int = 10,\n",
    "    megabatch_size: int = 512,\n",
    "    batch_size: int = 16,\n",
    "    iters_to_accumulate: int = 4,\n",
    "    num_workers: int = 16,\n",
    "    lr: float = 1e-6,  # Small learning rate for finetuning.\n",
    "    show_tqdm: bool = False,\n",
    "):\n",
    "    # NOTE: Need to pass bytes as the encoding scheme here, there seems to be some\n",
    "    # incompability between python 2/3 pickle. For more info see:\n",
    "    # https://stackoverflow.com/questions/11305790/pickle-incompatibility-of-numpy-arrays-between-python-2-and-3\n",
    "    word_to_idx, idx_to_word = pickle.load(open(vocab_path, 'rb'), encoding='bytes')\n",
    "\n",
    "    data_kwargs = {\n",
    "        'comics_data_path': comics_data_path,\n",
    "        'vgg_feats_path': vgg_feats_path,\n",
    "        'vocab_path': vocab_path,\n",
    "        'folds_dir': folds_dir,\n",
    "        'difficulty': difficulty,\n",
    "        'megabatch_size': megabatch_size,\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': True,\n",
    "    }\n",
    "    train_dataloader, train_dataset = make_dataloader(**data_kwargs, fold='train')\n",
    "    return train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe3110e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comics_data_path = '../11711_COMICS/data/comics.h5'\n",
    "vgg_feats_path  = '../11711_COMICS/data/vgg_features.h5'\n",
    "vocab_path  = '../11711_COMICS/data/comics_vocab.p'\n",
    "folds_dir  = '../11711_COMICS/folds'\n",
    "difficulty  = 'easy'\n",
    "n_epochs  = 10\n",
    "megabatch_size  = 512\n",
    "batch_size  = 16\n",
    "iters_to_accumulate  = 4\n",
    "num_workers  = 16\n",
    "lr  = 1e-6  # Small learning rate for finetuning.\n",
    "show_tqdm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75389d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ComicsDataset(\n",
    "        comics_data_path=comics_data_path,\n",
    "        vgg_feats_path=vgg_feats_path,\n",
    "        vocab_path=vocab_path,\n",
    "        folds_dir=folds_dir,\n",
    "        difficulty=difficulty,\n",
    "        fold='train',\n",
    "        batch_size=batch_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d046fec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[[0,1,2]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
